{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TitanicFromTs.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNaI69Lh4qkAdyjiCobb+0h"},"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit ('tensorflow': conda)","metadata":{"interpreter":{"hash":"af09b2e8d2c233d9eb67ea9dbf455d536f3d547df5db77af0e0e836762b2d079"}}}},"cells":[{"cell_type":"code","metadata":{"id":"gzn0iOeZfTSp"},"source":["# -*- coding: utf-8 -*-\n","#pip install jupyter\n","#pip install tensorflow\n","\n","import random\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf"],"execution_count":5,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32m<ipython-input-5-9ef658225457>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"]}]},{"cell_type":"code","metadata":{"id":"cANijBXtgBO8"},"source":["from sklearn.model_selection import train_test_split\r\n","from sklearn.feature_extraction import DictVectorizer\r\n","from sklearn import preprocessing"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O7djnxgoevUO"},"source":["# Parameters\n","learning_rate = 0.01 # 学習率 （※高いとcostの収束が早まる）\n","training_epochs = 10 # 世代数（※学習全体をこのエポック数で区切り、区切りごとにcostを表示する）\n","batch_size = 100     # 訓練単位（※学習1回ごと( sess.run()ごと )に訓練データをいくつ利用するか）\n","display_step = 1     # 1なら毎エポックごとにcostを表示\n","train_size = 800     # 全データの中でいくつ訓練データに回すか\n","step_size = 1000     # 何ステップ学習するか\n","\n","# Network Parameters\n","n_hidden_1 = 64      # 隠れ層1のユニットの数\n","n_hidden_2 = 64      # 隠れ層2のユニットの数\n","n_input = 8          # 与える変数の数\n","n_classes = 2        # 分類するクラスの数 今回は生き残ったか否かなので2\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"fkOhcfJnN-Cx","executionInfo":{"status":"error","timestamp":1612847890077,"user_tz":-540,"elapsed":723,"user":{"displayName":"Kotaro Ko","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiAyUOZBDxtURc4nVZqv-jNV03CxzI4Dwr1H-4K=s64","userId":"05265138308082330382"}},"outputId":"f1a73fdd-164c-4efe-e1b5-96f682cf333a"},"source":["train = train.read_csv('./data/train.csv')\n","\n","def extract_cabin_type(x):\n","  cabin = x['Cabin']\n","  if isinstance(cabin, str) and cabin[0] != 'T':\n","    return cabin[0]\n","  else:\n","    return np.nan   \n","train['CabinType'] = train.apply(extract_cabin_type, axis=1)\n","\n","def male_female_child(x):\n","  age = x['Age']\n","  sex = x['Sex']\n","  if age <= 15:\n","    return 'child'\n","  else:\n","    return sex\n","train['PersonType'] = train.apply(male_female_child,axis=1)\n","\n","\n","#データ整形\n","train[\"Embarked\"] = train[\"Embarked\"].replace(\"C\", 0).replace(\"Q\", 1).replace(\"S\", 2)\n","train[\"CabinType\"] = train[\"CabinType\"].replace(\"A\", 0).replace(\"B\", 1).replace(\"C\", 2).replace(\"D\", 3).replace(\"E\", 4).replace(\"F\", 5).replace(\"G\", 6)\n","train[\"Sex\"] = train[\"Sex\"].replace(\"male\", 0).replace(\"female\", 1)\n","train[\"PersonType\"] = train[\"PersonType\"].replace(\"male\", 0).replace(\"female\", 1).replace(\"child\", 2)\n","\n","#データ補完\n","train[\"Embarked\"] = train[\"Embarked\"].fillna(2)\n","train[\"CabinType\"] = train[\"CabinType\"].fillna(-1)\n","age_mean = pd.concat([train[\"Age\"], test[\"Age\"]]).mean()\n","fare_mean = pd.concat([train[\"Fare\"], test[\"Fare\"]]).mean()\n","train[\"Age\"] = train[\"Age\"].fillna(age_mean)\n","train[\"Fare\"] = train[\"Fare\"].fillna(fare_mean)\n","\n","print('訓練データの欠損値の個数\\n', train.isnull().sum())\n","print('-' * 40)\n","print('テストデータの欠損値の個数\\n', test.isnull().sum())"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","\n","x_np = np.array(df[['Age', 'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']].fillna(0))\n","d = df[['SurvivedText']].to_dict('record')\n","vectorizer = DictVectorizer(sparse=False)\n","y_np = vectorizer.fit_transform(d)\n","\n","[x_train, x_test] = np.vsplit(x_np, [train_size]) # 入力データを訓練データとテストデータに分ける\n","[y_train, y_test] = np.vsplit(y_np, [train_size]) # ラベルを訓練データをテストデータに分ける"]},{"cell_type":"code","metadata":{"id":"A9UkrZsQhId1"},"source":["\r\n","# tf Graph input\r\n","x = tf.placeholder(\"float\", [None, n_input])\r\n","y = tf.placeholder(\"float\", [None, n_classes])\r\n","\r\n","# Create model\r\n","def multilayer_perceptron(x, weights, biases):\r\n","    # Hidden layer with RELU activation\r\n","    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\r\n","    layer_1 = tf.nn.relu(layer_1)\r\n","    # Hidden layer with RELU activation\r\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\r\n","    layer_2 = tf.nn.relu(layer_2)\r\n","    # Output layer with linear activation\r\n","    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\r\n","    return out_layer\r\n","\r\n","# Store layers weight & bias\r\n","weights = {\r\n","    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\r\n","    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\r\n","    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\r\n","}\r\n","biases = {\r\n","    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\r\n","    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\r\n","    'out': tf.Variable(tf.random_normal([n_classes]))\r\n","}\r\n","\r\n","# Construct model\r\n","pred = multilayer_perceptron(x, weights, biases)\r\n","\r\n","# Define loss and optimizer\r\n","cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\r\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\n","\r\n","# Initializing the variables\r\n","init = tf.initialize_all_variables()\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C3RkFbMShKXk"},"source":["\r\n","# Launch the graph\r\n","with tf.Session() as sess:\r\n","    sess.run(init)\r\n","\r\n","    # Training cycle\r\n","    for epoch in range(training_epochs):\r\n","        avg_cost = 0.\r\n","\r\n","        # Loop over step_size\r\n","        for i in range(step_size):\r\n","            # 訓練データから batch_size で指定した数をランダムに取得\r\n","            ind = np.random.choice(batch_size, batch_size)\r\n","            x_train_batch = x_train[ind]\r\n","            y_train_batch = y_train[ind]\r\n","            # Run optimization op (backprop) and cost op (to get loss value)\r\n","            _, c = sess.run([optimizer, cost], feed_dict={x: x_train_batch,\r\n","                                                          y: y_train_batch})\r\n","            # Compute average loss\r\n","            avg_cost += c / step_size\r\n","        # Display logs per epoch step\r\n","        if epoch % display_step == 0:\r\n","            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\r\n","                \"{:.9f}\".format(avg_cost))\r\n","    print(\"Optimization Finished!\")\r\n","\r\n","    # Test model\r\n","    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\r\n","    # Calculate accuracy\r\n","    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\r\n","    print(\"Accuracy:\", accuracy.eval({x: x_test, y: y_test}))"],"execution_count":null,"outputs":[]}]}